\section{Future Work}\label{sec:future-work}
\subsection{Time based properties}
To extend the grammars presented in \autoref{tab:pf-grammar-user} and \autoref{tab:pf-grammar} even further it could be interesting to investigate the possibility to create policies based on time. More specifically a \emph{date} literal could be introduced, which includes both date and time. Furthermore a new attribute name could be created once a data resource is put in the repository, together with \emph{author} and \emph{self}. The new attribute could be called \emph{date of origin} and its value would be a \emph{date} literal that corresponds to the date and time of placement in the data repository. As such the mapping between attribute names and literals $T$ of $\PUT$ could be extended with:
\begin{align*}
    T = T \cup \textit{date of origin} \mapsto currentDate()
\end{align*}
It is assumed that the implementation implements a function \emph{currentDate()} that returns a \emph{date} literals corresponding to the current time and date of the system.

As an example, consider the following integrity policy that ensure that either the data resource itself or one of its dependencies was created prior to \nth{1} December 2019:
\begin{align*}
    \inte \eqdef \F \textit{date of origin} < \textit{\nth{1} December 2019}
\end{align*}
Note that date and time is somewhat liberally defined here.

\subsection{Minimise NBA}
\subsubsection{Reachability in Automata}\label{sec:reach}
In the model-checking automata-based algorithm described in \autoref{sec:methods} two automata are used; GNBA and NBA. The size of the GNBA will directly impact the size of the NBA and the size of the NBA will directly impact the size of the product transition system $TS \otimes NBA$. 

Let us look at why the size of a NBA $A$ is affected by the size of a GNBA $\mathcal{G}$. The size of the NBA is $|\mathcal{A}| = \mathcal{O}(|\mathcal{G}| \cdot |\mathcal{F}|)$ where $\mathcal{F}$ denotes the set of acceptance sets in $\mathcal{G}$~\cite[thm. 4.56]{baier2008principles}. 
Not necessarily every state will be reachable from one of the initial states in the GNBA $\mathcal{G}=(Q,2^{AP},\delta,Q_0,\mathcal{F})$. A solution to identify this and narrow down $\mathcal{G}$ could be to create a copy of $\mathcal{G}$ called $\mathcal{G}'=(Q',2^{AP},\delta,Q_0,\mathcal{F}')$, where $Q' \subseteq Q$ and $\mathcal{F}' \subseteq \mathcal{F}$. $Q$ and $\mathcal{F}$ could be determined by performing a DFS from every state $q \in Q_0$. During a search for every state visited, if the state has been visited continue the search otherwise add it to $Q'$. Furthermore if the state is in a acceptance set add the set to $\mathcal{F}$ and continue the search. Doing so will give a GNBA $\mathcal{G}'$ that only contains reachable states and acceptance set, thus only the reachable states are transformed into an equivalent NBA.

\subsubsection{Existing solutions}\label{sec:existing}
The solution for minimising NBAs presented in \autoref{sec:reach} is rather naive as it does not change the structure of the NBA but merely removes unreachable states. The method for creating the NBA presented in \autoref{sec:nba} results in an NBA that in worst-case yields an automaton with $\mathcal{O}(2^{|\phi|} \cdot |\mathcal{F}|)$ states, where $\mathcal{F}$ is the size of the set of acceptance sets in the corresponding GNBA. The work presented by~\cite{somenzi2000efficient} extends the work done by~\cite{gerth1995simple} and~\cite{daniele1999improved} for generating small Büchi automata from LTL formulae, i.e. \emph{not} reducing an existing NBA. The extensions include: rewrite rules to the formula before translation, apply boolean optimisation techniques to reduced number of generated states by the translation, and simplify the transition structure and acceptance conditions of the resulting Büchi automaton~\cite{somenzi2000efficient}. The performance measurements taken by~\cite{somenzi2000efficient} shows that their algorithm outperforms previously published algorithms on both random formulae and commonly used formulae from the literature. Their algorithm is implemented as a heuristic approximation as the optimal was considered too expensive. Given the performance of the algorithm it is worthwhile considering it for further improvement of the work presented in this paper.

Where the above suggestion is a complete replacement of the currently implemented method for generating NBAs, it could be sufficient to consider the approach~\cite{hopcroft1971n} presents. The algorithm minimise states of an NBA where the asymptotic running time is $\mathcal{O}(n \cdot log(n))$, $n$ is the number of input states. The running time is achieved partially by extensive use of list processing~\cite{hopcroft1971n}. A thing to note is that the algorithm is only practical for minimising states in finite automata of up to several thousand states~\cite{hopcroft1971n}. This could give rise to issues, if this were to be applied on an industrial level, however neither the current method is suitable.

Another well-known method for reducing automata is the \emph{bisimulation equivalence} method. The aim of the method is to construct an equivalent transition system to the original one, such that if every step of the original can be matched by one or more steps in the equivalent one~\cite{baier2008principles}. An example of original work that utilises this methods are (Fisler and Vardi, 1998)\cite{fisler1998a}, who revisit bisimulation in an automata-theoretic framework.

\subsection{Reduction of LTL formulae}\label{sec:min-ltl}
As mentioned in \autoref{sec:cache} the time complexity of the model-checking is exponential in the length of the LTL formulae, so a way to improve the performance of the system, is to construct an algorithm that reduces the length of LTL formulae. The algorithm could utilise the equivalence rules for LTL displayed in~\cite[Fig.~5.7]{baier2008principles} as well logical equivalence rules. Let us consider a small example where a UPF $\upf$ is constructed from the grammar in \autoref{tab:pf-grammar-user}. Note that it omits using expressions and functions, as they will be converted to atomic propositions when transforming UPF to IPF to LTL anyway and thus do not affect the length.
\begin{align*}
    \pf_u &\eqdef (a \imply b) \land (\lnot b \imply \G\F\G\G c)     &|\upf| = 8 & \\
    \pf_u &\eqdef (a \imply b) \land (\lnot b \imply \G\F\G c)       &|\upf| = 7 &\enskip \text{using idempotent laws} \\
    \pf_u &\eqdef (a \imply b) \land (\lnot b \imply \F\G c)         &|\upf| = 6 &\enskip \text{using absorption laws} \\
    \pf_u &\eqdef (\lnot a \lor b) \land (\lnot b \imply \F\G c)     &|\upf| = 7 &\enskip \text{using equivalence rules for implication} \\
    \pf_u &\eqdef (\lnot a \lor b) \land (\lnot \lnot b \lor \F\G c) &|\upf| = 8 &\enskip \text{using equivalence rules for implication} \\
    \pf_u &\eqdef (\lnot a \lor b) \land (b \lor \F\G c)             &|\upf| = 6 &\enskip \text{using double negation law} \\
    \pf_u &\eqdef b \lor (\lnot a \land \F\G c)                      &|\upf| = 5 &\enskip \text{using commutative and distributive laws}
\end{align*}
Let us call the expanded formula $\pf_{u_e}$. If we consider the length of the original formula $\upf$ and the expanded formula $\pf_{u_e}$ when their derived operators are replaced, i.e. \emph{implication}, \emph{or}, \emph{eventually} and \emph{always}, we get:
\begin{align}
    |\upf| = 18 \\
    |\pf_{u_e}| = 10
\end{align}
Another example and more simple example where no derived operators are used:
\begin{align*}
    \pf_{u_1} &\eqdef a \land a \land a \land a \land a \land a \land a \land a \land a \enskip &|\pf_{u_1}| = 8 & \\
    \pf_{u_1} &\eqdef a                                                                 \enskip &|\pf_{u_1}| = 1 &\enskip \text{using idempotent laws}
\end{align*}
From the two examples it is clear that there is some performance to gain, but it can be difficult to quantify exactly how much as the reduction of the length is very dependant on how the formulae are constructed.

As previously mentioned in \autoref{sec:existing} did \cite{somenzi2000efficient} rewrite the LTL formulae, using a set of equivalence rules, to eliminate redundancies and to reduce the size of the resulting automaton. Similar work is done by \cite{babiak2012ltl}, which propose a new set of equivalence rules to reduce the number of temporal operators. Finally \cite{etessami2000optimizing} describes a \emph{proof-theoretic} reduction, which consists of set of rewrite rules that are applied to LTL formulae recursively.

As suggested in \autoref{ex:avoid} are confidentiality policies of the form $\conf \eqdef self \imply \conf_i$ trivially true in data resources which it was not associated with during a put operation. Thus could the inheritance function be optimised to omit formulae of this form during inheritance.

\subsection{Caching of formula results}\label{sec:cache}
The current implementation does not perform any caching of whether or not policies holds under a data repository with respect to some data resource, that is for both confidentiality and integrity policies. The time complexity of the automata-based model-checking algorithm that was described in \autoref{sec:methods} is linear in the size of the transition system, i.e. the transformed data repository, and exponential in the length of the LTL formulae, i.e. the transformed PF~\cite{baier2008principles}. The most significant part here is the exponential growth in the length of the formulae. As the dependency tree\footnote{The tree that forms when following the dependencies of a data resource to their roots.} will only grow bigger as more resources are added the data repository, the time to determine if a inherited confidentiality policy holds will only increase. To overcome this caching could be introduced. As parts of the grammar in \autoref{tab:pf-grammar-user} have a strong relation to the data resource that is associated with the formulae as well as the subject of the context of execution, it is necessary to keep track of more information than just the results of the formulae being satisfied. The additional information includes the \emph{rsc} literal that is being putted or queried, the policy formula being either confidentiality or integrity, and \emph{usr} literal that is the subject of execution. This could form a sort of dictionary D, which could be defined as follows:
\begin{itemize}
    \item $D : R \times F \times U \rightharpoonup b$ is a partial mapping between data resources, policy formulae and \emph{usr} literals to \emph{bol} literals
\end{itemize}
After the first occurrence of the resource, policy and subject combination, the result will be cached for next time this exact combination occur. Using the proper data structure for this will allow constant time of the model-checking, as it will be a simple lookup.